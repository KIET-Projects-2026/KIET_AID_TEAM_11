{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7U_Mm5yWiY-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGHZJEi8WiV-",
        "outputId": "f09185c6-8307-46c1-fe10-1452fecd73f5"
      },
      "outputs": [],
      "source": [
        "with open(\"../data/medical_final_dataset.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"Loaded records:\", len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3D5hJMVWiTE",
        "outputId": "fe9ea129-efa7-412f-f613-d7ce2996c83c"
      },
      "outputs": [],
      "source": [
        "rag_docs = []\n",
        "\n",
        "for item in data:\n",
        "    text = item[\"response\"]\n",
        "    if item.get(\"context\"):\n",
        "        text = item[\"context\"] + \" \" + text\n",
        "\n",
        "    rag_docs.append({\n",
        "        \"text\": text,\n",
        "        \"metadata\": item.get(\"metadata\", {})\n",
        "    })\n",
        "\n",
        "print(\"RAG docs:\", len(rag_docs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8Un-D3PWiQW"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=250, overlap=40):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(\" \".join(words[start:end]))\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POSUFcQvWiNO",
        "outputId": "683a44fc-a335-466e-d7da-954698489115"
      },
      "outputs": [],
      "source": [
        "chunks = []\n",
        "\n",
        "for doc in rag_docs:\n",
        "    for c in chunk_text(doc[\"text\"]):\n",
        "        chunks.append({\n",
        "            \"text\": c,\n",
        "            \"metadata\": doc[\"metadata\"]\n",
        "        })\n",
        "\n",
        "print(\"Total chunks:\", len(chunks))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njj6-kqTWiKO",
        "outputId": "8c1aec1e-e2c3-4144-95b4-c27d24104dd9"
      },
      "outputs": [],
      "source": [
        "biobert_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(biobert_name)\n",
        "model = AutoModel.from_pretrained(biobert_name)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8RVODvuWiHO"
      },
      "outputs": [],
      "source": [
        "def mean_pooling(output, mask):\n",
        "    token_embeddings = output.last_hidden_state\n",
        "    mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return (token_embeddings * mask).sum(1) / mask.sum(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhV06iJIWiEW"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "def embed_chunks(chunks, batch_size=32):\n",
        "    embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "        batch = [c[\"text\"] for c in chunks[i:i+batch_size]]\n",
        "\n",
        "        enc = tokenizer(\n",
        "            batch,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(**enc)\n",
        "            emb = mean_pooling(out, enc[\"attention_mask\"])\n",
        "\n",
        "        embeddings.append(emb.cpu().numpy())\n",
        "\n",
        "    return np.vstack(embeddings).astype(\"float32\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsR9aFxyWiBG",
        "outputId": "c4145f8f-b325-42a9-a9fa-30a46dfa846e"
      },
      "outputs": [],
      "source": [
        "embeddings = embed_chunks(chunks, batch_size=32)\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IaFlGpHWh-G",
        "outputId": "45078a24-722d-4f12-efa3-5bf11ebfe4f5"
      },
      "outputs": [],
      "source": [
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"FAISS index size:\", index.ntotal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_DSePxlWh1G"
      },
      "outputs": [],
      "source": [
        "np.save(\"embeddings.npy\", embeddings)\n",
        "faiss.write_index(index, \"faiss.index\")\n",
        "\n",
        "with open(\"chunks.json\", \"w\") as f:\n",
        "    json.dump(chunks, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEhMpZD0Ykf3"
      },
      "outputs": [],
      "source": [
        "def extract_intent(question):\n",
        "    q = question.lower()\n",
        "    if \"symptom\" in q or \"sign\" in q:\n",
        "        return \"symptoms\"\n",
        "    if \"risk\" in q or \"cause\" in q:\n",
        "        return \"risk\"\n",
        "    if \"treatment\" in q:\n",
        "        return \"treatment\"\n",
        "    if \"what is\" in q or \"define\" in q:\n",
        "        return \"definition\"\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9nCjoBIYFQ1"
      },
      "outputs": [],
      "source": [
        "def intent_match(text, intent):\n",
        "    t = text.lower()\n",
        "\n",
        "    if intent == \"symptoms\":\n",
        "        return any(k in t for k in [\n",
        "            \"symptom\", \"sign\", \"cough\", \"shortness of breath\", \"pain\", \"fatigue\"\n",
        "        ])\n",
        "\n",
        "    if intent == \"risk\":\n",
        "        return any(k in t for k in [\n",
        "            \"risk\", \"smoking\", \"exposure\"\n",
        "        ])\n",
        "\n",
        "    if intent == \"treatment\":\n",
        "        return any(k in t for k in [\n",
        "            \"treatment\", \"surgery\", \"chemotherapy\", \"radiation\"\n",
        "        ])\n",
        "\n",
        "    if intent == \"definition\":\n",
        "        return \"is a disease\" in t or \"is a type\" in t\n",
        "\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcNGAuB8Yzgu"
      },
      "outputs": [],
      "source": [
        "def retrieve(query, top_k=10):\n",
        "    enc = tokenizer(\n",
        "        query,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**enc)\n",
        "        q_emb = mean_pooling(out, enc[\"attention_mask\"]).cpu().numpy()\n",
        "\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    _, idx = index.search(q_emb, top_k)\n",
        "\n",
        "    disease = extract_disease(query)\n",
        "    intent = extract_intent(query)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in idx[0]:\n",
        "        text = chunks[i][\"text\"]\n",
        "\n",
        "        # Disease filter\n",
        "        if disease and disease not in text.lower():\n",
        "            continue\n",
        "\n",
        "        # Intent filter\n",
        "        if intent and not intent_match(text, intent):\n",
        "            continue\n",
        "\n",
        "        results.append(text)\n",
        "\n",
        "        if len(results) >= 3:\n",
        "            break\n",
        "\n",
        "    return \"\\n\\n\".join(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDQr3jAUYFNf",
        "outputId": "27644e0b-0c71-4934-94b9-3dfc9666e87e"
      },
      "outputs": [],
      "source": [
        "print(retrieve(\"What are the symptoms of non-small cell lung cancer?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqI26ra2YFJ9",
        "outputId": "d16dd966-64da-489d-b5e5-a8344c215cad"
      },
      "outputs": [],
      "source": [
        "llama_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama_name)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_name,\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\").eval()\n",
        "\n",
        "print(next(llama_model.parameters()).device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTRwbr8najrt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def split_sentences(text):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oTPo-v_alF1"
      },
      "outputs": [],
      "source": [
        "def strict_context_filter(answer, context):\n",
        "    ctx_sentences = split_sentences(context.lower())\n",
        "    final = []\n",
        "\n",
        "    for sent in split_sentences(answer):\n",
        "        s = sent.lower()\n",
        "\n",
        "        # Keep ONLY if sentence is clearly grounded in context\n",
        "        if any(cs in s or s in cs for cs in ctx_sentences):\n",
        "            final.append(sent)\n",
        "\n",
        "    if not final:\n",
        "        return \"I don't have enough information.\"\n",
        "\n",
        "    # Max 2â€“3 sentences\n",
        "    return \" \".join(final[:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILIdIEObaoPO"
      },
      "outputs": [],
      "source": [
        "def build_prompt(context, question):\n",
        "    return f\"\"\"\n",
        "You are a medical assistant.\n",
        "\n",
        "TASK:\n",
        "Select the exact sentences from the context that answer the question.\n",
        "Do NOT add new information.\n",
        "Do NOT explain.\n",
        "Do NOT use lists.\n",
        "If the answer is not present, say: I don't have enough information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer (use only sentences from context):\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSgyFwf7aqNF"
      },
      "outputs": [],
      "source": [
        "def answer(question):\n",
        "    context = retrieve(question)\n",
        "\n",
        "    prompt = build_prompt(context, question)\n",
        "\n",
        "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = llama_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,       # VERY SMALL\n",
        "            do_sample=False,         # ðŸ”´ NO CREATIVITY\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=llama_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    raw = llama_tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    return strict_context_filter(raw, context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0fm3jhauYQ",
        "outputId": "7a5d5b96-fecc-4fcb-b7e3-bf9b90acfc2f"
      },
      "outputs": [],
      "source": [
        "print(answer(\"What are the symptoms of non-small cell lung cancer?\"))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
